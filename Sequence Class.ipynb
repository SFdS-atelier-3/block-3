{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequences and RNNs\n",
    "## Intro to Sequential Data and History of RNN\n",
    "### Sequential Data\n",
    "#### Time series\n",
    "#### Text processing\n",
    "##### Applications\n",
    "- Classification (Sentiment analysis, SPAM detection)\n",
    "- Translation\n",
    "- Captioning\n",
    "- Dialog\n",
    "### History of RNN\n",
    "#### Early stage 90s\n",
    "- Schmidhuber, Hochreiter, Bengio identify vanishing / exploding gradient problem\n",
    "- Hochreiter invents LSTM 1997\n",
    "\n",
    "#### 2013 2014 All Hell Breaks loose\n",
    "#### 2015 Attention\n",
    "#### 2017 RNNs are more and more declared obsolete\n",
    "### What constitutes an RNN\n",
    "- Diagrams of possible setups, gathered from places\n",
    "- Diagrams of possible internal processing\n",
    "$$h_{t+1} = \\rho(W_{hh}h_t + W_{xh}x_t)$$\n",
    "\n",
    "\n",
    "## Intro to Coding RNNs in Pytorch\n",
    "The goal of this part is to become familiar with the code building blocks that can be assembled into an RNN. From architecture specification, to training, from data preparation to evaluation nothing is left out.\n",
    "\n",
    "In a first step we identify common aspects between many RNNs and will bring them to life using some small examples which nevertheless show interesting properties.\n",
    "\n",
    "In a second step we shall work on bigger architectures for text analysis.\n",
    "\n",
    "### Architecture Blocks\n",
    "We will identify building blocks in typical pytorch RNN architectures that can be used to quickly set up an RNN and evaluate how they differ from other neural network architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `__init__` and `build` methods\n",
    "The `__init__` and `build` methods take care of storing parameters and setting up the data structures for training. While `__init__` should only store parameters (as per the useful `scikit-learn` conventions), `build` creates all the trainable `Parameter`s:\n",
    "\n",
    "```python\n",
    "def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "    super(ClassName, self).__init__()  # Pytorch requires this superclass init\n",
    "    self.input_dim = input_dim\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.output_dim = output_dim\n",
    "\n",
    "def build(self):\n",
    "    # Here we set up some parameters, for example:\n",
    "    self.input_to_hidden = torch.nn.Linear(self.input_dim, self.hidden_dim)\n",
    "    self.hidden_to_hidden = torch.nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "    self.hidden_to_output = torch.nn.Linear(self.hidden_dim, self.output_dim)\n",
    "```\n",
    "\n",
    "In this example, our build method creates three affine mappings, one from input to hidden, one from hidden to hidden, and one from hidden to output. These mappings are trainable and will be used by the RNN step logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `step` function\n",
    "The `step` function is a point in which RNNs differ from many other neural network architectures. In the step function, the current hidden state is manipulated by a specific operation, which may or may not involve using a data input and yielding an output.\n",
    "\n",
    "The step function is an important unit of core functionality of the RNN.\n",
    "\n",
    "Here we implement the (*fully linear*) step\n",
    "$$h_t = W_{hh}h_{t-1} + W_{xh}x_t$$\n",
    "$$y_t = W_{hy}h_t$$\n",
    "\n",
    "\n",
    "```python\n",
    "def step(self, x_in, hidden):\n",
    "    # Eg use the input and a hidden state to create a new hidden state and an output\n",
    "    new_hidden_from_hidden = self.hidden_to_hidden(hidden) \n",
    "    new_hidden_from_input = self.input_to_hidden(x_in)\n",
    "    new_hidden = new_hidden_from_hidden + new_hidden_from_input\n",
    "    y_out = self.hidden_to_output(new_hidden)\n",
    "    return y_out, new_hidden\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `forward` function\n",
    "This is the central function of a neural network module class. It should be able to take an input sample (or ideally a batch of samples) and output something that can be evaluated with a loss function.\n",
    "\n",
    "In the case of RNNs, the input sample is a sequence of something.\n",
    "\n",
    "The type of output can vary. For example, it could be a point-to point output predicting the next step.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "    # We need to assume a shape, e.g. (input_features, time)\n",
    "    n_features, n_time = x.shape\n",
    "    # initialize a hidden state:\n",
    "    hidden = initial_hidden_state\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    # Loop over input data and call step\n",
    "    for i in range(n_time):\n",
    "        # take time point\n",
    "        xx = x[:, i]\n",
    "        # do step\n",
    "        out, hidden = self.step(xx, hidden)\n",
    "        outputs.append(out)\n",
    "    \n",
    "    return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `train` and `test` functions\n",
    "The train function resembles those for most other neural networks. We usually split it into a function that trains one epoch of a data set and function that iterates over epochs.\n",
    "\n",
    "The function iterating over one train epoch with gradient updates takes a `model`, a `dataset`, an evalutation `criterion`, and an `optimizer`.\n",
    "\n",
    "```python\n",
    "def train_epoch(model, dataset, criterion, optimizer):\n",
    "    loss_values = []\n",
    "    for x, y in dataset:\n",
    "        prediction = model(x)\n",
    "        loss = criterion(prediction, y)\n",
    "        loss_values.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.mean(loss_values)\n",
    "```\n",
    "\n",
    "The function for evaluating on a test set looks quite similar\n",
    "```python\n",
    "def test(model, dataset, criterion):\n",
    "    loss_values = []\n",
    "    with torch.no_grad(): # switching of gradients makes things faster\n",
    "        for x, y in dataset:\n",
    "            prediction = model(x)\n",
    "            loss = criterion(prediction, y)\n",
    "            loss_values.append(loss.detach().cpu().numpy())\n",
    "    return np.mean(loss_values)\n",
    "```\n",
    "\n",
    "\n",
    "The function iterating over epochs could look like this:\n",
    "```python\n",
    "def train_mse_adam(model, dataset, n_epochs=10, test_dataset=None):\n",
    "    criterion = torch.nn.MSELoss() # We define the loss criterion here\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    for i in range(n_epochs):\n",
    "        train_loss = train_epoch(model, dataset, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        if test_dataset is not None:\n",
    "            test_loss = test(mode, test_dataset, criterion)\n",
    "            test_losses.append(test_loss)\n",
    "    return train_losses, test_losses\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In some cases, a `generate` function is useful\n",
    "Imagine you have trained an RNN to predict the next item of a sequence, but now you would like to use it to generate a new sequence from a starting point by predicting the next point and feeding that back in as input.\n",
    "\n",
    "Here is a way we can do this using the `step` function:\n",
    "```python\n",
    "def generate(rnn, starting_point, n):\n",
    "    # generate n time points by plugging output back into input\n",
    "    \n",
    "    hidden = self.initial_hidden_state\n",
    "    x = starting_point\n",
    "    outputs = []\n",
    "    for i in range(n):\n",
    "        x, hidden = rnn.step(x, hidden)\n",
    "        outputs.append(x)\n",
    "    return outputs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small RNNs\n",
    "With all this information about how to structure an RNN with pytorch, let's dive into some examples!\n",
    "\n",
    "In this section, we will go through a fairly detailed series of small neural networks, showing examples of the different types of RNN architectures, adding complexity along the way and getting familiar with the pytorch functionatities for RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some of the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A linear RNN that models sinusoids - simple dynamical system\n",
    "In this example we will generate a synthetic sinusoidal signal and see if we can model it with a very simple linear RNN. \n",
    "\n",
    "This is basically the simplest RNN possible and will get us started with the necessary scaffolding for setting up RNN training.\n",
    "\n",
    "It is not an uninteresting example though: Through experiments we will learn a few things about linear dynamical systems and RNN computational capacity.\n",
    "\n",
    "Architecture-wise we will be looking at two things:\n",
    "- A continuous-output network which outputs a value at every time step: The prediction of the next value\n",
    "- A short-circuiting of this process to create a generator\n",
    "\n",
    "As a reminder, this corresponds to the two following settings:\n",
    "\n",
    "<img src=\"images/rnn_many_to_many.png\" width=\"500\"></img>\n",
    "<img src=\"images/rnn_seq_to_seq.png\" width=\"450\"></img>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some sinusoidal data. In the next cell there is a function that can generate sinusoids of different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sin(n_samples, sin_frequencies=(1.,), weights=None, \n",
    "                 n_time_steps=100, sample_frequency=0.1, noise_level=0.):\n",
    "    time_vals = np.arange(0, n_time_steps) * sample_frequency\n",
    "    phase_offsets = np.random.rand(n_samples, len(sin_frequencies)) * 2 * np.pi\n",
    "    if weights is None:\n",
    "        weights = np.ones(len(sin_frequencies)) / len(sin_frequencies)\n",
    "    \n",
    "    sin_frequencies = np.asarray(sin_frequencies)\n",
    "    pure_sinusoids = np.sin(2 * np.pi * time_vals[:, np.newaxis] * sin_frequencies\n",
    "                           + phase_offsets[:, np.newaxis])\n",
    "    complex_sinusoids = pure_sinusoids.dot(weights)\n",
    "    return complex_sinusoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function by generating a few sinusoids of one frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinusoids = generate_sin(10, sin_frequencies=(1.,))\n",
    "plt.figure(figsize=(10, 1))\n",
    "plt.plot(sinusoids.T)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set up the linear RNN\n",
    "To set up the linear RNN, we can almost copy and paste the functions from the introductory description above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_input=1, n_hidden=1, n_output=1):\n",
    "        super(LinRNN, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_to_hidden = torch.nn.Linear(self.n_input, self.n_hidden)\n",
    "        self.hidden_to_hidden = torch.nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        self.hidden_to_output = torch.nn.Linear(self.n_hidden, self.n_output)\n",
    "        \n",
    "        self.initial_hidden_state = torch.nn.Parameter(torch.randn(self.n_hidden) * 0.01)\n",
    "    \n",
    "    def step(self, x, hidden):\n",
    "        # assume x.shape = (batch, features)\n",
    "        new_hidden = self.input_to_hidden(x) + self.hidden_to_hidden(hidden)\n",
    "        output = self.hidden_to_output(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x.shape = (batch, time, features)\n",
    "        B, T, F = x.shape\n",
    "        \n",
    "        hidden = self.initial_hidden_state\n",
    "        outputs = torch.empty((B, T, self.n_output))\n",
    "        for t in range(T):\n",
    "            xx = x[:, t, :]\n",
    "            out, hidden = self.step(xx, hidden)\n",
    "            outputs[:, t, :] = out\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the auxiliary functions for training and evaluating the network correspond to the prototypes from above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function that can perform one run through the full dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x, y in dataset:\n",
    "        p = model(x)\n",
    "        loss = criterion(p, y)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a function that can perform a test run. The difference to above is that no gradients are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, criterion):\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataset:\n",
    "            p = model(x)\n",
    "            loss = criterion(p, y)\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining the two above, we have the training function, which repeats the training process for a number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mse_adam(model, dataset, test_dataset=None, n_epochs=2):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for e in range(n_epochs):\n",
    "        train_loss = train_epoch(model, dataset, criterion, optimizer)\n",
    "        train_losses.append(train_loss)\n",
    "        if test_dataset is not None:\n",
    "            test_loss = test(model, test_dataset, criterion)\n",
    "            test_losses.append(test_loss)\n",
    "    if test_dataset is not None:\n",
    "        return np.array(train_losses), np.array(test_losses)\n",
    "    return np.array(train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have a little helper that can glue two datasets together. This is useful to create input and target sequences, because the target is just to input shifted by 1 (we are trying to predict the next step!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZipDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, *datasets):\n",
    "        self.datasets = datasets\n",
    "    def __len__(self):\n",
    "        return min(len(dataset) for dataset in self.datasets)\n",
    "    def __getitem__(self, i):\n",
    "        return tuple(dataset[i] for dataset in self.datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate some train and test data, which we convert to torch arrays. We add an axis to represent a 1D input feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(generate_sin(1000)[:, :, np.newaxis].astype('float32'))\n",
    "test_data = torch.from_numpy(generate_sin(100)[:, :, np.newaxis].astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our input and target data by simply shifting the train data on the time axis, zipping the two together using our zipper helper, and using the torch `DataLoader` class to create an object that yields batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(ZipDataset(train_data[:, :-1], train_data[:, 1:]),\n",
    "                                               batch_size=2, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(ZipDataset(test_data[:, :-1], test_data[:, 1:]),\n",
    "                                              batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an RNN. Remember the constructor from above? It is `LinRNN(n_input=1, n_hidden=1, n_output=1)`. Let's create one that takes 1 input value and yields 1 output value and uses 1 hidden unit. These are exactly the default values from above, but it doesn't hurt to specify them here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnn = LinRNN(n_hidden=1, n_input=1, n_output=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = train_mse_adam(lrnn, train_dataloader, test_dataloader, n_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process outputs both train and test losses in their raw, batched form. Their shapes are `(n_epochs, n_batches_per_epoch)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss.shape, test_loss.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People often average the loss over batches to obtain just one value per epoch. In our case it might be instructive  to take a look at the full extent of the loss batches, because the optimization is actually quite fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training loss curves. We can plot each epoch separately by plotting the transpose of the train loss array (`matplotlib` plots matrix columns as lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss.T)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also concatenate all the losses and get a more global picture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(np.concatenate(train_loss))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can take a look at the performance on the test set in basically the same way. Note that the test set batches were bigger, and there were fewer points in total, so the whole curve has much fewer points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(np.concatenate(test_loss))\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test loss gets evaluated after every training batch. This is why we see such a steppy function: The performance is just much better after one full train epoch has passed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we would like to take a look at some individual predictions. We can visually evaluate how well it does at the task that it was trained on - predict the next element of the sequence.\n",
    "\n",
    "Let's take a sample from the test data for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot it, just to be sure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(test_sample.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we push it through the RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_next = lrnn(test_sample[np.newaxis]).detach().cpu().numpy().ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can take a look at the prediction compared to the sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(test_sample_next, label='next')\n",
    "plt.plot(test_sample.numpy().ravel(), label='current')\n",
    "plt.legend()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wait ..., so the blue curve (*\"next\"*) is **in front of** the orange one? Why is this?\n",
    "- What else do you observe? How could one address this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation through generation\n",
    "\n",
    "Since this model predicts the next item of the sequence, we have another interesting check: We can use the output of one time point as the input for the next! \n",
    "\n",
    "This corresponds to the second diagram above, where we reuse the network predictions as input. Note that this is a harder evaluation: Not only was the network not trained to do this task, but also \"looking into itself\" might lead to some weird recurrent behavior.\n",
    "\n",
    "Let's check to see what happens when we do that. We start with an initial time point, grabbed from the test sample, and the initial hidden state of the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = test_sample[np.newaxis, 0, :]\n",
    "hidden = lrnn.initial_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these two value we can feed the `step` function of our neural network. This will give an output and a new hidden state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, lrnn.step(x, hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this in a loop: Feed `x` and the current `hidden` into the step function, and play the output back into `x` and `hidden`. Keep track of `x` and do this a few times to generate a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = [x]\n",
    "for i in range(100):\n",
    "    x, hidden = lrnn.step(x, hidden)\n",
    "    generated.append(x.detach().cpu().numpy())\n",
    "generated = np.array(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, so that worked less well. Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise:\n",
    "- play around with the number of hidden states to see if you can fix this\n",
    "- if you manage to fix it: why do you think it works now?\n",
    "- try more complex sinusoids by adding frequencies to `generate_sin`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A tanh one that can replay or reverse a short sequence of a small alphabet\n",
    "Now that we have trained a linear RNN on continuous data, let's move on to something else: an RNN that works on symbols. Since linear networks probably won't work for this, let's directly make a nonlinear one using a `tanh` nonlinearity.\n",
    "\n",
    "Also, note that we have to come up with a way of knowing which symbol to output, and also how to input symbols into RNNs.\n",
    "\n",
    "The answer to this is *one-hot-encoding*, whereby every symbol is embedded into a space of dimensionality corresponding to the total number of symbols. Each symbol is assigned to one of the cartesian dimensions in that space and is thus represented by a vector of mostly zeros, except for one 1 in one of the entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/onehot.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will write a network to perform the so-called \"copy task\", that is: parse an input sequence with an encoder RNN into a hidden state, and then decode the original sequence or its reversed order in the output.\n",
    "\n",
    "This task will permit us to understand some properties of simple symbol RNNs and the technicalities of training them. It will provide a starting point for incorporating built-in RNN modules to make code more concise, standardized, and faster.\n",
    "\n",
    "It's architecture is of the following type\n",
    "<img src=\"images/rnn_input_to_latent_output_from_latent.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g.: **the &nbsp;&nbsp;&nbsp;&nbsp;cat &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sat  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; on &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  the &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   mat** --> **the &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  cat  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; sat &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  on  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; the  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; mat**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's start by creating some data to get an idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with an alphabet, which we just define as a list of symbols. Let's use a small one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = np.array(list('abc'))\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this alphabet, we can generate random symbol sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sequences = 1000\n",
    "sequence_length = 10\n",
    "sequences = np.random.choice(alphabet, (n_sequences, sequence_length))\n",
    "sequences[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(side note: the `dtype` of this `numpy` array, `<U1` means that this array contains entries of unicode strings of length less than or equal to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to give these sequences to an RNN, we need to put them into vector format. This is done by *One-Hot* encoding as mentioned before.\n",
    "\n",
    "In order to create a one-hot encoded array quickly and easily, we can use the *implicit broadcasting* of numpy: We add an axis to our 2D array `sequences`, making it a 3D array, where the last entry of the shape is `1`. We then check each entry of the sequences for equality with each entry of the alphabet. This will broadcast the last axis to be of the size of the alphabet.\n",
    "After this is done, we convert the output to `float32`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_one_hot = (sequences[..., np.newaxis] == alphabet).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be useful to take a moment to break every step down:\n",
    "\n",
    "- Add an axis to `sequences`, check shape\n",
    "- Add two empty axes to `alphabet`, check shape\n",
    "- Perform equality\n",
    "- Use the fact that implicit broadcasting *prepends as many 1s as needed* to the shorter-shaped array and drop the adding of axes to `alphabet`\n",
    "- convert the binary output to floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check the first two one-hot-encoded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_one_hot[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create an encoder RNN\n",
    "We now set about building the first half of the encoder-decoder network, the encoder.\n",
    "\n",
    "We create an RNN just like the linear one above, with two modifications:\n",
    "- we add a nonlinearity, then hyperbolic tangent;\n",
    "- we make the RNN output only the last hidden state.\n",
    "\n",
    "The first point is accomplished by using `torch.tanh` in the `step` function. The second point is done by simply returning only `hidden` from `forward` after the for loop ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoder(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(SeqEncoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_to_hidden = torch.nn.Linear(self.n_input, self.n_hidden)\n",
    "        self.hidden_to_hidden = torch.nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        \n",
    "        self.initial_hidden_state = torch.nn.Parameter(torch.randn(self.n_hidden) * 0.01)\n",
    "\n",
    "    def step(self, x, hidden):\n",
    "        new_hidden = torch.tanh(self.input_to_hidden(x) + self.hidden_to_hidden(hidden))\n",
    "        return new_hidden\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x.shape = batch, time, features\n",
    "        batch_size, time_length, n_features = x.shape\n",
    "        hidden = self.initial_hidden_state * torch.ones(batch_size, 1).to(self.initial_hidden_state.device)\n",
    "        \n",
    "        for i in range(time_length):\n",
    "            xx = x[:, i]\n",
    "            hidden = self.step(xx, hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give it a quick try, by instantiating an encoder with 3 input dimensions (corresponding to alphabet length) and 10 hidden units. Any input sequences should end up encoded as a 10-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoder = SeqEncoder(n_input=3, n_hidden=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some of our one-hot-encoded sequences from before to check this. Let's input two of them. We should get an output of shape `(2, 10)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = seq_encoder.forward(torch.from_numpy(sequences_one_hot[:2]))\n",
    "encoded.shape, encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, this yields two 10-dimensional hidden-state vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a decoder RNN\n",
    "\n",
    "The second half of an encoder/decoder network is the decoder. It is an RNN that takes an initial hidden state, but no inputs, and just iterates hidden state and output using its weights.\n",
    "\n",
    "To create this, we modify the step function from the above encoder to only take a hidden state as input (no input sequence is used) and return a new hidden state and an output.\n",
    "\n",
    "The forward function iterates this step over hidden states and gathers all the outputs, which it then returns. It is necessary to add an indication of how long we want the output sequence to be, so we put and argument `n` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDecoder(torch.nn.Module):\n",
    "    def __init__(self, n_hidden, n_output):\n",
    "        super(SeqDecoder, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.hidden_to_hidden = torch.nn.Linear(self.n_hidden, self.n_hidden)\n",
    "        self.hidden_to_output = torch.nn.Linear(self.n_hidden, self.n_output)\n",
    "    \n",
    "    def step(self, hidden):\n",
    "        new_hidden = torch.tanh(self.hidden_to_hidden(hidden))\n",
    "        output = self.hidden_to_output(new_hidden)\n",
    "        return output, new_hidden\n",
    "    \n",
    "    def forward(self, initial_hidden, n):\n",
    "        batch_size, n_hidden = initial_hidden.shape\n",
    "        \n",
    "        hidden = initial_hidden\n",
    "        outputs = torch.zeros(batch_size, n, self.n_output)\n",
    "        for i in range(n):\n",
    "            output, hidden = self.step(hidden)\n",
    "            outputs[:, i] = output\n",
    "        \n",
    "        return outputs\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give this decoder a try immediately. We can create one with 10 hidden units and 3 output units. This corresponds to the hidden state of before and the alphabet length of our sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_decoder = SeqDecoder(10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the input to the encoder gave us a shape of `(2, 10)` for 2 sequences and their hidden state? Let's use something of that shape for our decoder and ask for a sequence of length 5 as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = seq_decoder(torch.randn(2, 10), 5)\n",
    "output.shape, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the output is of shape `(2, 5, 3)` for 2 examples, length 5 and alphabet length 3.\n",
    "\n",
    "We can also try decoding the variable `encoded` from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_decoder(encoded, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that just concluded a full step of encoding and decoding a sequence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now let's create an encoder-decoder RNN combining the two\n",
    "The next step is to create an object that does both encoding and decoding, so that we can train it with sequence data. We will thus create a small pytorch module containing an encoder and a decoder. All it does is book-keeping: Given an input dimension (alphabet length) and a hidden dimension, it can build an encoder and a decoder. The `forward` function takes an input sequence `x`, encodes it, and then decodes a sequences of exactly the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoderDecoder(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SeqEncoderDecoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.encoder = SeqEncoder(self.input_dim, self.hidden_dim)\n",
    "        self.decoder = SeqDecoder(self.hidden_dim, self.input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, time_length, _ = x.shape\n",
    "        hidden = self.encoder(x)\n",
    "        output = self.decoder(hidden, time_length)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's push some data through it. The input shape should correspond with the output shape.\n",
    "\n",
    "We instantiate and encoder-decoder with 3 input units and 10 hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoder_decoder = SeqEncoderDecoder(3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we put our two initial sequences through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_decoded = seq_encoder_decoder(torch.from_numpy(sequences_one_hot[:2]))\n",
    "encoded_decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both input and output have the same shape, and can now be compared to one another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_decoded.shape, sequences_one_hot[:2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a loss function to optimize\n",
    "\n",
    "How do we go about defining a loss function that makes our RNN understand that its output should be close to its input, for all inputs?\n",
    "\n",
    "We could use MSE as for the sinusoid, but recall that our inputs and our outputs are supposed to reflect categorical variables. An appropriate differentiable loss for categorical variables is *cross entropy*, which is the loglikelihood of classification problems. It is defined as\n",
    "$$\\mathbb E_p[\\log q]$$\n",
    "for two distributions $p$ and $q$ over categories. In our case we are dealing with an estimate of this expectation, where the variable $p$ represents the one-hot-encoded \"sure\" probability of a certain class and $q$ represents the more uncertain prediction, which is the softmax of our neural network outputs:\n",
    "$$q_j(y) = \\textrm{softmax}(y)_j = \\frac{\\exp y_j}{\\sum_k\\exp y_k}$$\n",
    "\n",
    "The estimator of the loglikelihood estimated from samples indexed with $i$ writes as\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N \\sum_{j=1}^M p^i_j\\log q^i_j,$$\n",
    "where $M$ is the number of targets.\n",
    "\n",
    "Given that $p_i$ is just a one-hot code indicating which label $j$ was active, by calling the label $l_i$, this can be rewritten as\n",
    "$$\\frac{1}{N}\\sum_{i=1}^N \\log q^i_{l_j}.$$\n",
    "\n",
    "Basically this is indexing the output variable with the label. Maximizing this quantity will make the label more probable.\n",
    "\n",
    "It is also in this indexing way that `pytorch` built-in loss functions are written out.\n",
    "\n",
    "To use these mechanisms, we need our target variable (the one we compare to) to be a number indicating which dimension was correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create that here by restarting from scratch with our sequences, using the `np.unique` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet, isequences = np.unique(sequences, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using `return_inverse=True`, we not only get the (sorted) alphabet used, but also full sequences of indices into that alphabet representing which of the symbols was used in each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet, isequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could get the sequences back by fancy-indexing one with the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet[isequences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that `np.unique` destroys the shape of the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet[isequences].shape, sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to set it back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isequences.shape = sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, we can now one-hot-encode these sequences to obtain neural network input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isequences_one_hot = (isequences[..., np.newaxis] == np.arange(len(alphabet))).astype('float32')\n",
    "isequences_one_hot[:2], isequences[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we also have the exact index sequence, which we can use for the loss:\n",
    "\n",
    "We are sure we have a correspondence between `isequences` and `isequences_one_hot`.\n",
    "\n",
    "This being fixed, we can now look at the `CrossEntropyLoss` object of `pytorch` and figure out what inputs it requires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at its docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's try it out!\n",
    "\n",
    "we encode and decode two of our one-hot-encoded sequences and get one-hot predictions out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_decoded = seq_encoder_decoder(torch.from_numpy(isequences_one_hot[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isequences_one_hot.shape, encoded_decoded.shape, isequences_one_hot[:2], encoded_decoded[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the criterion on our predictions, and the indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion(encoded_decoded, torch.from_numpy(isequences[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above errors because of a shape problem (look at debugger!)\n",
    "\n",
    "Below you see the shapes of `encoded_decoded` and the target `isequences[:2]`. It seems the above criterion is iterating over the wrong axis (it seems to iterate over `axis=1` since the shape it extracts is `2, 3`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_decoded.shape, isequences[:2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take another look at the docstring of the function to figure out what might be wrong!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** What do we need to change to make this code work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we permute the axes, we should make the criterion understand our data better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_decoded.permute(0, 2, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(encoded_decoded.permute(0, 2, 1), torch.from_numpy(isequences[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transposing the output, we get a negative loglikelihood loss which we can use in gradient descent!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating a dataset iterator\n",
    "\n",
    "Now that we have obtained a working evaluation criterion, next we need to create a dataset that consists of one-hot encoded sequences as input, and the corresponding integer sequences as output.\n",
    "\n",
    "Let's inherit from the `torch.utils.data.Dataset` object, have it store some arbitrary sequences, and incorporate functionality that a) creates the sequences of unique integers b) the corresponding one-hot-encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.alphabet, self.isequences = np.unique(self.sequences, return_inverse=True)\n",
    "        self.isequences.shape = self.sequences.shape\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        seq = self.isequences[i]\n",
    "        one_hot = (seq[..., np.newaxis] == np.arange(len(self.alphabet))).astype('float32')\n",
    "        return torch.from_numpy(one_hot), torch.from_numpy(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out on our sequences. Remember what they looked like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, here goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dataset = SeqDataset(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset object permits indexing (in general just with integers, but the way we implemented, it also works with slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_dataset[10:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset object outputs our input sequences one-hot encoded and our target indices as tuples. This is the perfect object to hand to `torch.utils.data.DataLoader`, which can collect this into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.utils.data.DataLoader(seq_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out one of these batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data:\n",
    "    break # stop loop after one iteration, just to fill x and y once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, it handed us input and output for 32 sequences of length 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a loss and an optimizer and do some training\n",
    "We create the usual objects for training a neural network. \n",
    "\n",
    "- The network\n",
    "- an optimization criterion\n",
    "- an optimizer\n",
    "\n",
    "We already have the train data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_encoder_decoder = SeqEncoderDecoder(len(alphabet), 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the loss function needs to swap some axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda x, y: torch.nn.CrossEntropyLoss()(x.permute(0, 2, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good first-try optimizer is Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(seq_encoder_decoder.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train this for some epochs!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []\n",
    "for i in range(100):\n",
    "    losses = train_epoch(seq_encoder_decoder, data, criterion, optimizer)\n",
    "    all_losses.append(losses)\n",
    "all_losses = np.array(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 2))\n",
    "plt.plot(np.concatenate(all_losses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging by the training loss decrease, it looks like the neural network \"gets it\"!\n",
    "\n",
    "**Suggestion:** Try to look at the plot with the y-axis in log scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check this on some sequences\n",
    "We'll now generate some test sequences by hand and see the predictions of our network\n",
    "\n",
    "First we generate some random test sequences of length `sequence_length` from our alphabet (we could check whether they are already in the train set or not, but at least some of them won't be, and it's hard for a model so small to memorize all train sequences and be bad only on test sequences). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = np.random.choice(alphabet, (100, sequence_length))\n",
    "test_sequences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create the one-hot-encoded versions of the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.from_numpy((test_sequences[..., np.newaxis] == alphabet).astype('float32'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the test sequences through our encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = seq_encoder_decoder(test_input).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to read the sequences in their original alphabet, we convert them in two steps: First we find the strongest-activated dimension in the alphabet axis, and then we look up the corresponding letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_sequences = alphabet[test_output.argmax(axis=2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output_sequences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it correspond? Let's check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_output_sequences == test_sequences).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the output corresponds perfectly to the input sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise 02:\n",
    "**1\\.** Make it learn to output the sequence backwards. You can do this by creating a `SeqDataset` object that has a flag indicating whether the output should be reversed, and have it feed the reversed prediction target in.\n",
    "(Hint: Just copy and paste most of the above and make a little modification to `SeqDataset`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rnn/solution02.1.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-3/master/solutions/rnn/solution02.1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Make a bigger alphabet, vary sequence length at training. See how many hidden units you need to accommodate.\n",
    "\n",
    "*This takes some for loops and will probably be to time-consuming for the class, so we will probably skip it*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Feed a sequence of different length through an RNN trained for length 10. Evaluate the encoding/decoding performance for sequences of length 2 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rnn/solution02.3.txt\n",
    "# %load https://raw.githubusercontent.com/SFdS-atelier-3/block-3/master/solutions/rnn/solution02.3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh, well that is awkward! The error is zero only at exactly the sequence length we trained the RNN with. Looks like our instructions were very precise (\"learn to copy sequences of length 10\") and followed *very precisely*...\n",
    "\n",
    "RNNs develop their power in treating sequences of arbitrary length, so it would be really nice if our example here could benefit from that. In the process of fixing this we will learn about an important concept: extra symbols for end of sequence.\n",
    "\n",
    "(But: What happens if we just train this thing on different-length sequences? Will it forget stuff?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Introducing \"punctuation\" for RNNs\n",
    "\n",
    "RNNs are powerful sequence processors, but we need to make sure to be very precise on how we train them. The above example shows that we cannot expect an RNN that we have trained to copy or reverse sequences of length 10 will be able to do so for any other number except 10. This is surprising at first, but when one accepts that RNNs just try to minimize their losses, one can imagine that it might do the first best thing, not them \"human generalized\" thing we had in mind.\n",
    "\n",
    "In order to train variable-length sequences, we can add symbols to our sequences that tell the network to stop iterating.\n",
    "\n",
    "The concept is easily explained: We add symbols to our sequences which signify certain operations. Right now we need to indicate \"end of sequence\". Let's reserve 0 for that for all sequences\n",
    "\n",
    "There are two levels of complexity to implementing this\n",
    "\n",
    "1. One could just decide on what the longest sequence length should be, and pad all shorter sequences with the stop symbol. This will probably alleviate the problem above a bit.\n",
    "2. It becomes quite apparent though that sequences with too much stop symbol in them might degrade even before reaching the end latent state. Also the output should not have the burden to output lots of stop symbols. Enter loss masks for the output, and selecting the last hidden state before stop symbol of the input.\n",
    "\n",
    "We will check out both of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating an alphabet, and generating a list of random integer sequence lengths, from which we then generate a list of different-length random sequences of alphabet symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = np.array(list('abc'))\n",
    "n_sequences = 10000\n",
    "sequence_lengths = np.random.randint(3, 15, (n_sequences,))\n",
    "sequences = [np.random.choice(alphabet, sequence_length) for sequence_length in sequence_lengths]\n",
    "sequence_lengths.sum(), sequence_lengths.mean(), sequences[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above numbers show: total number of symbols, average number of symbols per sequence, and three different sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the alphabet/integer codes pair on all the sequences (by concatenating them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_alphabet, i_sequences = np.unique(np.concatenate(sequences), return_inverse=True)\n",
    "u_alphabet, i_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our sequence lengths are different for each sample, we need to place them into a 2D array which can accommodate the maximum sequence, and pad all the others with zeros. \n",
    "\n",
    "In order to figure out where to place the values, let's first make a mask using some smart broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_mask = sequence_lengths[:, np.newaxis] > np.arange(max(sequence_lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sequence length in the column vector `sequence_lengths[:, np.newaxis]` we obtain a boolean vector in which the first n entries are True, where n in the current sequence length, and the remaining entries are False.\n",
    "\n",
    "Its shape is `(number of sequences, maximum sequence length)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a feeling for what this looks like, we can display it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(length_mask, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use this mask to place our `isequences` into a square array. \n",
    "All we have to do is use *array masking* to place the sequences at the appropriate locations. If we add 1 to `i_sequences`, then our non-stop-symbols are `1, 2, 3` and our stop-symbol can be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first make an integer array of the same size as our mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = np.zeros_like(length_mask, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we assign the sequences (indices shifted by 1) to this square array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences[length_mask] = i_sequences + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at these padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(padded_sequences, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train an *encoder-decoder* copying RNN with that, exactly like before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our procedure carries over immediately, since the sequences we input are not very much different from the ones before, except that there is a strong prevalence for zeros at the end of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our `SeqDataset` object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_seq_dataset = SeqDataset(padded_sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_data = torch.utils.data.DataLoader(var_seq_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And a network that takes input size one bigger than the alphabet length in order to be able to accommodate the stop symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_seq_encoder_decoder = SeqEncoderDecoder(len(alphabet) + 1, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need an optimization criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda x, y: torch.nn.CrossEntropyLoss()(x.permute(0, 2, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we need an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(var_seq_encoder_decoder.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train that network for a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):  # 200 does work better\n",
    "    var_losses.append(train_epoch(var_seq_encoder_decoder, var_data, criterion, optimizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's take a look at how well the network trained this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(var_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much much worse! What is going on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a visual of how the predictions look by iterating through the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = []\n",
    "for x, y in var_seq_dataset:\n",
    "    train_output.append(var_seq_encoder_decoder(x[np.newaxis]).detach().cpu().numpy().argmax(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the output as an image of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.array(train_output)[:, 0], aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to visually assess the quality of these predictions, we can check whether they correspond with the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.array(train_output)[:, 0] == var_seq_dataset.sequences, aspect='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that they correspond at the beginning (not always, depending on training) and at the end. The end is mostly the padding with the stop symbol, which means that the RNN guessed the sequence length right. Since it gets rewarded equally for each one of these predictions, predicting a constant slab of stop symbols already gets you pretty far. Getting the first few right also helps towards the prediction goal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the accuracy of the predictions outside the stop symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(train_output)[:, 0] == var_seq_dataset.sequences)[length_mask].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect, but not completely random either."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we included all the stop symbols, then our predictive accuracy would be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(train_output)[:, 0] == var_seq_dataset.sequences).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some actual predicted symbol sequences. Here is our ordered alphabet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an element to it: the empty string. By placing it at index 0, it will catch all stop symbols and they won't appear when the prediction is concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_alphabet = np.concatenate([[''], u_alphabet])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loop through the sequences, and show the true sequence on the left and the predicted sequence on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, m in zip(modified_alphabet[var_seq_dataset.sequences], \n",
    "                modified_alphabet[np.array(train_output)[:, 0]]):\n",
    "    print(\"\".join(l), len(\"\".join(l)), \"-\", \"\".join(m), len(\"\".join(m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to have gotten the length right very consistently, but is making errors with the actual symbols. The content is not completely wrong, but could be better.\n",
    "\n",
    "We can speculate that the RNN was just too occupied to get the 0s at the end correctly predicted.\n",
    "\n",
    "Making the network bigger and giving it more epochs to train shows that it does get better at memorizing the sequence, but also that the loss sometimes becomes very big again and needs to decrease from the beginning (probably due to strange loss shape)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using the built-in RNN module\n",
    "Before we move on to addressing these issues,  let's introduce something helpful -- a built-in `pytorch` RNN module, that can shorten our code.\n",
    "\n",
    "Let's take a moment now to explore what `pytorch` can offer us in terms of pre-made objects.\n",
    "\n",
    "Once an understanding is acquired, using pre-made objects for RNNs has multiple advantages:\n",
    "- they permit writing less code\n",
    "- other people can understand it better\n",
    "- they might be optimized and run faster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the object `torch.nn.RNN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn = torch.nn.RNN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(input_size=4, hidden_size=10, batch_first=True, num_layers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = rnn.forward(torch.randn(2, 5, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0].shape, out[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By matching shapes we conclude that the first output is the hidden state. According to the docstring the second output is the last hidden state. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[0][:, -1] - out[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why report both of these?\n",
    "\n",
    "It starts making a difference when we set `num_layers > 1`. Then the last state will be given across all layers, but the rest will only give the output of the last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's add a 1-layer `RNN` object into our encoder and decoder classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoderRNN(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(SeqEncoderRNN, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.rnn = torch.nn.RNN(input_size=self.n_input,\n",
    "                                hidden_size=self.n_hidden,\n",
    "                                num_layers=1, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # assume x.shape = batch, time, features\n",
    "        out, hidden = self.rnn(x)\n",
    "        return hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, n_hidden, n_output):\n",
    "        super(SeqDecoderRNN, self).__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.rnn = torch.nn.RNN(input_size=1, hidden_size=self.n_hidden, batch_first=True)\n",
    "        self.hidden_to_output = torch.nn.Linear(self.n_hidden, self.n_output)\n",
    "    \n",
    "    def forward(self, initial_hidden, n):\n",
    "        batch_size, n_hidden = initial_hidden.shape\n",
    "        \n",
    "        # input a bunch of zeros, because the RNN requires input\n",
    "        inputs = torch.zeros(batch_size, n, 1, device=initial_hidden.device)\n",
    "        outputs, hidden = self.rnn(inputs, initial_hidden[np.newaxis])\n",
    "        \n",
    "        # outputs will be of size batch, time, n_hidden\n",
    "        # need to linear transform them to output size\n",
    "        \n",
    "        out = self.hidden_to_output(outputs.reshape(batch_size * n, self.n_hidden)\n",
    "                                   ).reshape(batch_size, n, self.n_output)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoderDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SeqEncoderDecoderRNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.encoder = SeqEncoderRNN(self.input_dim, self.hidden_dim)\n",
    "        self.decoder = SeqDecoderRNN(self.hidden_dim, self.input_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, time_length, _ = x.shape\n",
    "        hidden = self.encoder(x)\n",
    "        output = self.decoder(hidden, time_length)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks a bit shorter than before.\n",
    "\n",
    "Another step to making this shorter would be to integrate everything into `SeqEncoderDecoderRNN`\n",
    "\n",
    "Let's train this to see that we get similar results as before, and then move on to masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_seq_encoder_decoder_rnn = SeqEncoderDecoderRNN(len(alphabet) + 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda x, y: torch.nn.CrossEntropyLoss()(x.permute(0, 2, 1), y)\n",
    "optimizer = torch.optim.Adam(var_seq_encoder_decoder_rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_losses = []\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): #100\n",
    "    var_losses.append(train_epoch(var_seq_encoder_decoder_rnn, var_data, criterion, optimizer))\n",
    "    print(\".\", end=\"\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(var_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = []\n",
    "for x, y in var_seq_dataset:\n",
    "    train_output.append(var_seq_encoder_decoder_rnn(x[np.newaxis]).detach().cpu().numpy().argmax(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.array(train_output)[:, 0], aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(np.array(train_output)[:, 0] - var_seq_dataset.sequences, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(train_output)[:, 0] == var_seq_dataset.sequences)[length_mask].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_seq_dataset.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modified_alphabet = np.concatenate([[''], u_alphabet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, m in zip(modified_alphabet[var_seq_dataset.sequences], \n",
    "                modified_alphabet[np.array(train_output)[:, 0]]):\n",
    "    print(\"\".join(l), len(\"\".join(l)), \"-\", \"\".join(m), len(\"\".join(m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using masks to restrict to region of interest\n",
    "\n",
    "We now introduce a way of restricting the learning to the parts of the sequences that are not stop symbols. This will take us towards general and efficient ways of training RNNs.\n",
    "\n",
    "Here we modify our architecture at two points:\n",
    "1. at the encoder level, we do not wait until the end of the sequence to return the hidden state, but rather return the hidden state upon the first encounter of a stop symbol (i.e. when the sequence is considered over)\n",
    "2. at the training level, we only care about the loss up to the first stop symbol in the target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoderRNNMasked(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(SeqEncoderRNNMasked, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.rnn = torch.nn.RNN(input_size=self.n_input,\n",
    "                                hidden_size=self.n_hidden,\n",
    "                                num_layers=1, batch_first=True)\n",
    "    \n",
    "    def forward(self, x, seq_lengths):\n",
    "        # assume x.shape = batch, time, features\n",
    "        out, hidden = self.rnn(x)\n",
    "        # take advantage of the fact that for 1-layer rnn\n",
    "        # out gives you the hidden states\n",
    "        return out[torch.arange(x.shape[0]), seq_lengths.reshape(-1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqEncoderDecoderRNNMasked(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(SeqEncoderDecoderRNNMasked, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.encoder = SeqEncoderRNNMasked(self.input_dim, self.hidden_dim)\n",
    "        self.decoder = SeqDecoderRNN(self.hidden_dim, self.input_dim)\n",
    "    \n",
    "    def forward(self, x, seq_lengths):\n",
    "        _, time_length, _ = x.shape\n",
    "        \n",
    "        hidden = self.encoder(x, seq_lengths)\n",
    "        output = self.decoder(hidden, time_length)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarSeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.sequence_lengths = np.array(list(map(len, self.sequences)))\n",
    "        self.alphabet, self.isequences = np.unique(np.concatenate(self.sequences), return_inverse=True)\n",
    "        self.mask = self.sequence_lengths[:, np.newaxis] > np.arange(self.sequence_lengths.max() + 1)\n",
    "        self.padded_sequences = np.zeros_like(self.mask, dtype='int')\n",
    "        self.padded_sequences[self.mask] = self.isequences + 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        seq = self.padded_sequences[i]\n",
    "        one_hot = (seq[..., np.newaxis] == np.arange(len(self.alphabet) + 1)).astype('float32')\n",
    "        return torch.from_numpy(one_hot), torch.from_numpy(seq), torch.from_numpy(self.sequence_lengths[i:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsd = VarSeqDataset(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsd[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_var(model, dataset, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x, y, l in dataset:\n",
    "        p = model(x, l)\n",
    "        loss = criterion(p, y)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sedrm = SeqEncoderDecoderRNNMasked(len(alphabet) + 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(sedrm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_len_data = torch.utils.data.DataLoader(vsd, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_len_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): #100\n",
    "    var_len_losses.append(train_epoch_var(sedrm, var_len_data, criterion, optimizer))\n",
    "    print(\".\", end=\"\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(var_len_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = []\n",
    "for x, y, l in vsd:\n",
    "    train_output.append(sedrm(x[np.newaxis], l).detach().cpu().numpy().argmax(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output = np.vstack(train_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(train_output - vsd.padded_sequences, aspect='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for l, m in zip(modified_alphabet[vsd.padded_sequences], \n",
    "                modified_alphabet[train_output]):\n",
    "    print(\"\".join(l), len(\"\".join(l)), \"-\", \"\".join(m), len(\"\".join(m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add in a training function that cuts the loss at the first stop symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_var_smart(model, dataset, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x, y, l in dataset:\n",
    "        p = model(x, l)\n",
    "        \n",
    "        mask = l >= torch.arange(y.shape[1])\n",
    "        \n",
    "        loss = criterion(p[mask], y[mask])\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sedrm2 = SeqEncoderDecoderRNNMasked(len(alphabet) + 1, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(sedrm2.parameters())\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_len_data = torch.utils.data.DataLoader(vsd, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_len_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20): # 100\n",
    "    var_len_losses.append(train_epoch_var_smart(sedrm2, var_len_data, criterion, optimizer))\n",
    "    print(\".\", end=\"\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(var_len_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A LSTM one that can do additions\n",
    "\n",
    "Now that we have gotten to grips with the plumbing of RNNs using the very simple copy task, let's apply our knowledge to an example that is slightly less trivial: Learning arithmetic, specifically addition, on text sequences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's first come up with a way of creating data\n",
    "\n",
    "Let's come up with a way of generating many many integer addition examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import check_random_state\n",
    "def generate_additions_x_plus_y(n_examples, min_len_x=3, max_len_x=4, min_len_y=3, max_len_y=4, random_state=0):\n",
    "    # note that this creates most numbers of the max length\n",
    "    rng = check_random_state(random_state)\n",
    "    min_x = 10 ** (min_len_x - 1)\n",
    "    max_x = 10 ** max_len_x - 1\n",
    "    min_y = 10 ** (min_len_y - 1)\n",
    "    max_y = 10 ** max_len_y - 1\n",
    "    \n",
    "    xs = rng.randint(min_x, max_x + 1, n_examples)\n",
    "    ys = rng.randint(min_y, max_y + 1, n_examples)\n",
    "    sums = [(f\"{x}+{y}\", f\"{x + y}\") for x, y in zip(xs, ys)]\n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums, outputs = zip(*generate_additions_x_plus_y(20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_dataset = VarSeqDataset(list(map(list, sums)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_dataset[361]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dataset = VarSeqDataset(list(map(list, outputs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumRNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden, n_layers=1):\n",
    "        super(SumRNN, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.encoder_rnn = torch.nn.RNN(input_size=self.n_inputs,\n",
    "                                        hidden_size=self.n_hidden,\n",
    "                                        num_layers=self.n_layers,\n",
    "                                        batch_first=True)\n",
    "        self.decoder_rnn = torch.nn.RNN(input_size=1,\n",
    "                                        hidden_size=self.n_hidden,\n",
    "                                        num_layers = self.n_layers,\n",
    "                                        batch_first=True)\n",
    "        self.hidden_to_output = torch.nn.Linear(self.n_hidden,\n",
    "                                                self.n_outputs)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        out, hidden = self.encoder_rnn(x)\n",
    "        return hidden\n",
    "    \n",
    "    def decode(self, initial_hidden, n):\n",
    "        n_layers, batch_size, n_hidden = initial_hidden.shape\n",
    "        \n",
    "        zero_input = torch.zeros(batch_size, n, 1, device=initial_hidden.device)\n",
    "        out, hidden = self.decoder_rnn(zero_input, initial_hidden)\n",
    "        \n",
    "        output = self.hidden_to_output(\n",
    "            out.reshape(batch_size * n, n_hidden)).reshape(batch_size, n, self.n_outputs)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded, x.shape[1])\n",
    "        return decoded\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums_outputs = ZipDataset(sums_dataset, outputs_dataset)\n",
    "sums_dataloader = torch.utils.data.DataLoader(sums_outputs, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_rnn = SumRNN(len(sums_dataset.alphabet) + 1, len(outputs_dataset.alphabet) + 1, 100, n_layers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda x,y: torch.nn.CrossEntropyLoss()(x.permute(0, 2, 1), y)\n",
    "optimizer = torch.optim.Adam(sum_rnn.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch1(model, dataset, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x, y in dataset:\n",
    "        p = model(x[0])\n",
    "        loss = criterion(p[:, :y[0].shape[1]], y[1])\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sum_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(20): # 20 epochs takes 10min on laptop\n",
    "    losses = train_epoch1(sum_rnn, sums_dataloader, criterion, optimizer)\n",
    "    all_sum_losses.append(losses)\n",
    "    print(\".\", end=\"\")\n",
    "    sys.stdout.flush()\n",
    "t1 = time.time()\n",
    "print(f\"\\nThis took {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(all_sum_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = [sum_rnn(x[0][np.newaxis]).detach().cpu().numpy().argmax(-1) for x in sums_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_alphabet_aug = np.concatenate([[''], outputs_dataset.alphabet])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_symbols = output_alphabet_aug[predictions]\n",
    "[\"\".join(line) for line in predictions_symbols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, o, p in zip(sums, outputs, predictions_symbols):\n",
    "    print(s, \"=\", o, \"?\", \"\".join(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's replace the simple RNN by an LSTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumLSTM(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, n_inputs, n_outputs, n_hidden, n_layers=1):\n",
    "        super(SumLSTM, self).__init__()\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.encoder_rnn = torch.nn.LSTM(input_size=self.n_inputs,\n",
    "                                        hidden_size=self.n_hidden,\n",
    "                                        num_layers=self.n_layers,\n",
    "                                        batch_first=True)\n",
    "        self.decoder_rnn = torch.nn.LSTM(input_size=1,\n",
    "                                        hidden_size=self.n_hidden,\n",
    "                                        num_layers = self.n_layers,\n",
    "                                        batch_first=True)\n",
    "        self.hidden_to_output = torch.nn.Linear(self.n_hidden,\n",
    "                                                self.n_outputs)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        out, hidden = self.encoder_rnn(x)\n",
    "        return hidden\n",
    "    \n",
    "    def decode(self, initial_hidden, n):\n",
    "        n_layers, batch_size, n_hidden = initial_hidden[0].shape\n",
    "        \n",
    "        zero_input = torch.zeros(batch_size, n, 1, device=initial_hidden[0].device)\n",
    "        out, hidden = self.decoder_rnn(zero_input, initial_hidden)\n",
    "        \n",
    "        output = self.hidden_to_output(\n",
    "            out.reshape(batch_size * n, n_hidden)).reshape(batch_size, n, self.n_outputs)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encode(x)\n",
    "        decoded = self.decode(encoded, x.shape[1])\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_lstm = SumLSTM(len(sums_dataset.alphabet) + 1, len(outputs_dataset.alphabet) + 1, 200, n_layers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda x,y: torch.nn.CrossEntropyLoss()(x.permute(0, 2, 1), y)\n",
    "optimizer = torch.optim.Adam(sum_lstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch1(model, dataset, criterion, optimizer):\n",
    "    losses = []\n",
    "    for x, y in dataset:\n",
    "        p = model(x[0])\n",
    "        loss = criterion(p[:, :y[0].shape[1]], y[1])\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sum_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "for i in range(10):\n",
    "    losses = train_epoch1(sum_lstm, sums_dataloader, criterion, optimizer)\n",
    "    all_sum_losses.append(losses)\n",
    "    print(\".\", end=\"\")\n",
    "    sys.stdout.flush()\n",
    "t1 = time.time()\n",
    "print(f\"\\nThis took {t1-t0}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(all_sum_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions = [sum_lstm(x[0][np.newaxis]).detach().cpu().numpy().argmax(-1) for x in sums_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_symbols = output_alphabet_aug[predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, o, p in zip(sums, outputs, predictions_symbols):\n",
    "    print(s, \"=\", o, \"?\", \"\".join(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger RNNs\n",
    "Now let's move on to some real-life examples\n",
    "#### A character-level predictive RNN\n",
    "Following Andrej Karpathy's famous blog post, we will implement an RNN that can predict the next character in a string. For this, let's first find an appropriate text.\n",
    "\n",
    "Let's download something from the Gutenberg project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!if [ ! -f ./moliere1.txt ]; then wget http://www.gutenberg.org/files/40086/40086-0.txt -O ./moliere1.txt; fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"./moliere.txt\"):\n",
    "    import urllib.request\n",
    "    urllib.request.urlretrieve(\"http://www.gutenberg.org/files/40086/40086-0.txt\", \"./moliere1.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's take a look at the beginning of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./moliere1.txt\", \"rb\") as f:\n",
    "    content = f.read().decode()\n",
    "content = content[31500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A glimpse at the unique characters in this text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet, icontent = np.unique(list(content), return_inverse=True)\n",
    "alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(icontent)\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.bar(np.arange(len(counts)), counts)\n",
    "plt.xticks(np.arange(len(counts)), alphabet)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now proceed to implement a neural network exactly like our sine-wave predictor, but for characters. We will use the LSTM out of the box in order to have as much representative power as possible.\n",
    "\n",
    "First we need a dataset that turns our text as subsequences of a length we specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, text, seq_length=100):\n",
    "        self.text = text\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.alphabet, self.itext = np.unique(list(self.text), return_inverse=True)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.text) - self.seq_length + 1\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.itext[i:i + self.seq_length] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement another dataset that takes a text dataset and makes it return two copies of each sequence, shifted by 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftByOne(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset) - 1\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.dataset[i], self.dataset[i + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = TextDataset(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifted_text_data = ShiftByOne(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataloader = torch.utils.data.DataLoader(shifted_text_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(text_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data.alphabet[x.numpy() - 1], text_data.alphabet[y.numpy() - 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of performing one-hot encoding, we will work with a direct embedding. This can be seen as an extra linear transform on top of the one-hot-encoded input. It permits us to adjust input dimensionality for large vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorLSTM(torch.nn.Module):\n",
    "    def __init__(self, n_symbols, n_hidden, n_layers=1, embedding_size=None):\n",
    "        super(PredictorLSTM, self).__init__()\n",
    "        self.n_symbols = n_symbols\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        if self.embedding_size is None:\n",
    "            embedding_size = self.n_symbols\n",
    "        else:\n",
    "            embedding_size = self.embedding_size\n",
    "        self.embeddings = torch.nn.Parameter(torch.randn(self.n_symbols, embedding_size) * 0.01)\n",
    "        self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=self.n_hidden,\n",
    "                                batch_first=True, num_layers=self.n_layers)\n",
    "        self.hidden_to_output = torch.nn.Linear(self.n_hidden, self.n_symbols)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded = self.embeddings[x]\n",
    "        out, cell_hidden = self.rnn(embedded)\n",
    "        output = self.hidden_to_output(out.reshape(-1, self.n_hidden)).reshape(out.shape[:2] + (self.n_symbols,))\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plstm = PredictorLSTM(n_symbols=len(text_data.alphabet) + 1, n_hidden=512, embedding_size=64, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = lambda x, y: torch.nn.CrossEntropyLoss()(x.permute(0, 2, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(plstm.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = plstm.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataset, criterion, optimizer, max_batches=None):\n",
    "    losses = []\n",
    "    if max_batches is None:\n",
    "        max_batches = len(dataset)\n",
    "    for i, (x, y) in zip(range(max_batches), dataset):\n",
    "        p = model(x)\n",
    "        loss = criterion(p, y)\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return np.array(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit all_losses.append(train_epoch(plstm, text_dataloader, criterion, optimizer, max_batches=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.concatenate(all_losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now for predictions.\n",
    "\n",
    "If you remember the way we did this for the sine wave: We put in some initial state, push it through the network and get an output. This output we feed in to obtain the next step.\n",
    "\n",
    "The pytorch LSTM module does not allow super easy prediction that way, so we have to do it ourselves. One way of doing this is to feed an increasingly larger array, adding on each prediction as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_letters = torch.arange(35, 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_array = torch.zeros((len(initial_letters), 1000), dtype=torch.long)\n",
    "prediction_array[:, 0] = initial_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, prediction_array.shape[1]):\n",
    "    pred_logits = plstm.forward(prediction_array[:, :i])[:, -1, :].detach()\n",
    "    pred_prob = torch.nn.Softmax(dim=1)(pred_logits)\n",
    "    samples = torch.multinomial(pred_prob, 1)\n",
    "    prediction_array[:, i] = samples[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(\"\".join(row)) for row in text_data.alphabet[prediction_array.numpy() - 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### But also a dictionary-based Markov model\n",
    "\n",
    "As a response to Andrej Karpathy's blog post about *The Unreasonable Effectiveness of Recurrent Neural Networks*, Yoav Goldberg showed that a very simple N-back Markov model is an extremely impressive baseline.\n",
    "\n",
    "The idea is to learn a sparse conditional probability distribution of a letter given N previous letters.\n",
    "\n",
    "We can easily learn this with a dictionary of lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markov_dict(text, markov_order=4):\n",
    "    \n",
    "    unique_characters = np.unique(list(text))\n",
    "    n_unique = len(unique_characters)\n",
    "    transl = dict(list(zip(unique_characters, np.arange(n_unique))) +\n",
    "                  list(zip(np.arange(n_unique), unique_characters)))\n",
    "    markov_dict = dict()\n",
    "    for i in range(len(text) - markov_order - 1):\n",
    "        key = text[i:i + markov_order]\n",
    "        value = text[i + markov_order]\n",
    "        freq_table = markov_dict[key] = markov_dict.get(key, np.zeros(n_unique, dtype='int'))\n",
    "        freq_table[transl[value]] += 1\n",
    "    return markov_dict, transl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d, t = create_markov_dict(content, markov_order=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate text from this dictionary by starting off with some characters that appear in the dictionary, and randomly sampling from the possibilities of the next letter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_from_markov(markov_dict, transl_dict, n=1000, starting_point=None):\n",
    "    \n",
    "    n_states = len(transl_dict) // 2\n",
    "    if starting_point is None:\n",
    "        starting_point = np.random.choice(list(markov_dict.keys()))\n",
    "    \n",
    "    output = list(starting_point)\n",
    "    cur_key = starting_point\n",
    "    \n",
    "    for i in range(n):\n",
    "        counts = markov_dict[cur_key]\n",
    "        counts = counts / counts.sum()\n",
    "        next_char_ind = np.random.multinomial(1, counts).argmax()\n",
    "        next_char = transl_dict[next_char_ind]\n",
    "        output.append(next_char)\n",
    "        \n",
    "        cur_key = cur_key[1:] + next_char\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated = generate_from_markov(d, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
